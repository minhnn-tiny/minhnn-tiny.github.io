@article{10.1145/3765751,
author = {Nguyen, Nhat-Minh and Nguyen, Hoang H. and Thanh, Long Le and Ahmadi, Zahra and Doan, Thanh-Nam and Wu, Daoyuan and Jiang, Lingxiao},
title = {MANDO-LLM: Heterogeneous Graph Transformers with Large Language Models for Smart Contract Vulnerability Detection},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3765751},
doi = {10.1145/3765751},
abstract = {Detecting vulnerabilities in smart contracts is vital for the security and reliability of decentralized apps. To facilitate vulnerability detection, contract codes, including bug patterns, are represented as heterogeneous graphs with various nodes and edges, like control-flow and function-call graphs. However, existing graph learning techniques struggle with large, complex graphs. This paper presents MANDO-LLM, a novel framework that combines heterogeneous graph transformers (HGTs) with large language models (LLMs) for detecting vulnerabilities in smart contracts represented as heterogeneous contract graphs built upon control-flow and call graphs. MANDO-LLM uses LLMs to capture code features from control-flow and call data, customizes HGTs to learn embeddings with specific node-edge meta relations, and employs classifiers for vulnerability detection in Solidity code at both contract and line levels. Our evaluation shows that MANDO-LLM significantly outperforms existing methods on real-world large-scale imbalanced datasets, with F1-score improvements from 0.59\% to 80.72\% at the contract level. It is also one of the first effective methods for identifying line-level vulnerabilities, with performance boosts ranging from 3.09\% to over 95\% across different vulnerability types. MANDO-LLMâ€™s versatility allows easy retraining for various vulnerabilities without needing manually defined patterns.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {vulnerability detection, smart contracts, source code, heterogeneous graph learning, graph transformer, graph embedding, large language model, code embedding}
}